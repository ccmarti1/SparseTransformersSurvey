<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Scaling Sparse Transformers for HPC</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Roboto', sans-serif;
      margin: 0;
      padding: 0;
      background-color: #ffffff;
      color: #333;
      line-height: 1.6;
    }
    header {
      background-color: #1f3c88;
      color: #fff;
      padding: 2rem 1rem;
      text-align: center;
    }
    header h1 {
      margin: 0;
      font-size: 2rem;
    }
    nav {
      margin-top: 1rem;
    }
    nav a {
      color: #ffe600;
      text-decoration: none;
      margin: 0 1rem;
      font-weight: bold;
    }
    main {
      padding: 2rem;
      max-width: 900px;
      margin: auto;
    }
    section {
      margin-bottom: 2rem;
    }
    footer {
      background-color: #f4f4f4;
      text-align: center;
      padding: 1rem;
      font-size: 0.9rem;
      color: #666;
    }
  </style>
</head>
<body>

<header>
  <h1>Scaling Sparse Transformers for High-Performance Computing</h1>
  <nav>
    <a href="#abstract">Abstract</a>
    <a href="#links">Quick Links</a>
    <a href="#overview">Overview</a>
    <a href="#contact">Contact</a>
    <a href="SurveyPaper_Webpage.htm" target="_blank">Full Paper</a>
  </nav>
</header>

<main>
  <section id="abstract">
    <h2>Abstract</h2>
    <p>Transformer models have revolutionized machine learning, but their quadratic O(LÂ²) attention complexity presents major scalability and efficiency challenges for large-scale applications, particularly in high-performance computing (HPC) environments. This survey systematically reviews and classifies over 20 recent works addressing Sparse Transformers and related optimization techniques. We categorize solutions into five key areas: sparse attention mechanisms, computational complexity reductions, software and compiler-level enhancements, hardware-aware sparse computation beyond attention, and real-world HPC deployments. For each category, we provide tutorial-style explanations and highlight methods that enable practical scaling to long-context, trillion-parameter models across GPUs, TPUs, and heterogeneous architectures. We further discuss open research challenges, integration with large language models (LLMs), and how this survey differs from existing reviews. Our goal is to bridge the gap between theoretical sparsity advances and their deployment in modern HPC systems, providing a roadmap for future developments in efficient Transformer scaling.</p>
  </section>

  <section id="links">
    <h2>Quick Links</h2>
    <ul>
      <li><a href="FullSurveyPaper.pdf" target="_blank">Download Full Paper (PDF)</a></li>
      <li><a href="SurveyPaper_Webpage.htm" target="_blank">View Full Paper Online (HTML)</a></li>
      <li><a href="References.html" target="_blank">Reference Library (HTML)</a></li>
    </ul>
  </section>

  <section id="overview">
    <h2>Overview</h2>
    <p>This project systematically classifies Sparse Transformer advancements across five key areas: sparse attention, complexity reductions, software/compiler optimizations, hardware-aware sparsity, and real-world HPC deployments.</p>
  </section>

  <section id="contact">
    <h2>Contact</h2>
    <p>For questions or collaborations: <a href="mailto:your_email@example.com">your_email@example.com</a></p>
    <p><a href="https://github.com/your_github_username">GitHub Profile</a></p>
  </section>
</main>

<footer>
  <p>&copy; 2025 [Your Name]. Powered by GitHub Pages.</p>
</footer>

</body>
</html>
